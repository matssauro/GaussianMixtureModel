{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58d786f",
   "metadata": {},
   "source": [
    "# Modèle de **mélanges gaussiens** pour la génération de données\n",
    "## 1 Modèle de mélange\n",
    "\n",
    "* Soit $Z\\in\\{1,\\dots,K\\}$ telle que $\\mathbb{P}[Z=k]=\\pi_k$ avec $\\sum_{k=1}^K\\pi_k=1$ ;\n",
    "* Soit $X\\in\\mathbb R^d$ telle que, conditionnellement à $Z=k$,  \n",
    "  $$X\\mid Z=k\\;\\sim\\;\\mathcal N(\\mu_k,\\Sigma_k)$$  \n",
    "  où $\\mu_k\\in\\mathbb R^d$ et $\\Sigma_k\\in\\mathbb R^{d\\times d}$ est définie positive.\n",
    "* On considère un modèle de mélange de gaussien definit (pour un K fixé) par:\n",
    " $$\\theta = (\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = ((\\pi_1, ..., \\pi_K),(\\mu_1, ..., \\mu_K),(\\Sigma_1, ..., \\Sigma_K))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd9d95",
   "metadata": {},
   "source": [
    "### T1\n",
    "---  \n",
    "On sait que $\\mathbb{P}[Z=k] = \\pi_k$\n",
    "\n",
    "Soit $f_k(x;\\mu_k,\\Sigma_k) = \\frac{1}{(2\\pi)^{k/2}\\,|\\Sigma|^{1/2}}\\,\n",
    "\\exp\\!\\bigl[-\\tfrac12\\,(x-\\mu)^\\top \\Sigma^{-1}(x-\\mu)\\bigr],\\qquad x \\in \\mathbb R$\n",
    "\n",
    "Donc, $f_k(x;\\mu_k,\\Sigma_k) = f_{X\\mid Z = k}(x )$\n",
    "  \n",
    "Par la loi totale de probabilité appliquée aux densités :\n",
    "\n",
    "$f_X(x)=\\sum_{k=1}^K \\mathbb{P}[Z=k] \\; f_{X\\mid Z}(x\\mid k)=\\sum_{k=1}^K \\pi_l f_{\\mu_k,\\Sigma_l}(x).$\n",
    "   \n",
    "La somme finie de densités, pondérée par des poids positifs qui somment à 1, est bien une densité (positive partout et intégrale égale à 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af866f9",
   "metadata": {},
   "source": [
    "### S1\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a89457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "import math\n",
    "import sklearn.datasets\n",
    "\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixtureModel:\n",
    "    d: int\n",
    "    weights: np.ndarray\n",
    "    means: np.ndarray\n",
    "    covariances: np.ndarray\n",
    "    def __init__(self, d):\n",
    "        self.d = d\n",
    "    \n",
    "    def set_params(self, weights, means, covariances):\n",
    "        if len(means[0]) != self.d or len(covariances[0]) != self.d:\n",
    "            raise ValueError(f\"Length of weights, means, and covariances must match {self.d}.\")\n",
    "        if len(weights) != len(means) or len(weights) != len(covariances):\n",
    "            raise ValueError(\"Weights, means, and covariances must have the same number of components.\")\n",
    "        self.weights = np.array(weights)\n",
    "        self.means = np.array(means)\n",
    "        self.covariances = np.array(covariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multivariate_normal(K, theta, n):\n",
    "    Z = rng.choice(K, size=n, p=theta.weights) # K will be 1 unity less than the real index of the model \n",
    "    X = np.zeros((n, K - 1))\n",
    "    for k in range(K):\n",
    "        indices = np.where(Z == k)[0]\n",
    "        for index in indices:\n",
    "            X[index] = rng.multivariate_normal(theta.means[k], theta.covariances[k])\n",
    "    return X, Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e12e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixtureModel(d=1)\n",
    "gmm.set_params(\n",
    "    weights=[0.3, 0.7],\n",
    "    means=[[0], [3]], \n",
    "    covariances=np.array([[[0.6]], [[2.0]]])\n",
    ")\n",
    "data, Z = plot_multivariate_normal(2, gmm, 100000)\n",
    "plt.hist(data, bins=100, density=True)\n",
    "X1 = data[Z == 0]\n",
    "X2 = data[Z == 1]\n",
    "plt.hist(X1, bins=100, density=True, alpha=0.3, label='Component 1')\n",
    "plt.hist(X2, bins=100, density=True, alpha=0.3, label='Component 2')\n",
    "plt.title('Multivariate Normal Samples from GMM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff99c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixtureModel(d=2)\n",
    "gmm.set_params(\n",
    "    weights=[0.3, 0.3, 0.4],\n",
    "    means=[[0, 0], [2, 2], [-1, 1.5]], \n",
    "    covariances=np.array([[[0.6, 0], [0, 0.6]], [[3.0, 0], [0, 3.0]], [[0.5, 0], [0, 0.5]]])\n",
    ")\n",
    "data, Z = plot_multivariate_normal(3, gmm, 100000)\n",
    "# plt.hist(X[:,0], bins=100, density=True)\n",
    "# plt.title('distribution of first dimension for d = 2')\n",
    "# plt.show()\n",
    "plt.hist2d(data[:,0], data[:,1], bins=(80,80), density=True, cmap = plt.cm.nipy_spectral)\n",
    "plt.colorbar()\n",
    "plt.title('2D Histogram of Multivariate Normal Samples from GMM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e34cd5",
   "metadata": {},
   "source": [
    "## 2 Algorithme d'estimation\n",
    "\n",
    "* Soit $Z\\in\\{1,\\dots,K\\}$ telle que $\\mathbb{P}[Z=k]=\\pi_k$ avec $\\pi_k0$ et $\\sum_{k=1}^K\\pi_k=1$ ;\n",
    "* Soit $X\\in\\mathbb R^d$ telle que, conditionnellement à $Z=k$,  \n",
    "  $$X\\mid Z=k\\;\\sim\\;\\mathcal N(\\mu_k,\\Sigma_k)$$  \n",
    "  où $\\mu_k\\in\\mathbb R^d$ et $\\Sigma_k\\in\\mathbb R^{d\\times d}$ est définie positive.\n",
    "* On considère un modèle de mélange de gaussien definit (pour un K fixé) par:\n",
    " $$\\theta = (\\boldsymbol{\\pi}, \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = ((\\pi_1, ..., \\pi_K),(\\mu_1, ..., \\mu_K),(\\Sigma_1, ..., \\Sigma_K))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604465c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vraisemblance(k, X, theta):\n",
    "    vraisemblance = 0.0\n",
    "    for j in range(k):\n",
    "        multinormal = multivariate_normal(mean=theta.means[j], cov=theta.covariances[j])\n",
    "        for point in X:\n",
    "            vraisemblance += math.log(multinormal.pdf(point) * theta.weights[j])\n",
    "\n",
    "\n",
    "    return vraisemblance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab1d3e3",
   "metadata": {},
   "source": [
    "### T2\n",
    "\n",
    "Calcul de la densité $p_{\\theta}$ du couple $(Z,X)$:\n",
    "\n",
    "Par définition, pour  $k \\in \\{1, ... , K\\}$  la variable  $X \\mid Z = k$  suit une loi normale de paramètre  $(\\mu_{k},\\Sigma_{k})$ de fonction de répartition $f_{\\mu_k, \\Sigma_k}$.\n",
    "\n",
    "Soit $k \\in \\{1, ..., K\\}$  et  $x \\in \\mathbb{R}$\n",
    "\n",
    "Par définition de la fonction densité conditionnelle : $$f_{X \\mid Z = k}(x) = \\frac{p_{\\theta}(k,x)}{\\mathbb{P}_{\\theta}(Z = k)}$$\n",
    "\n",
    "Or $\\mathbb{P}_{\\theta}(Z = k) = \\pi_k$\n",
    "\n",
    "Donc $$p_{\\theta}(k,x) = \\pi_k f_{\\mu_k,\\Sigma_k}(x)$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16422498",
   "metadata": {},
   "source": [
    "### T3\n",
    "\n",
    "Soit $k \\in \\{1, ..., K\\}$  et  $x \\in \\mathbb{R}$\n",
    "\n",
    "Par définition  $\\mathbb(P)_{\\theta}(Z = k \\mid X = x) = \\frac{p_{\\theta}(k,x)}{f_X(x)}$\n",
    "\n",
    "On substitue alors les expressions au numérateur et au dénominateur obtenues respectivement en T2 et T1.\n",
    "\n",
    "Ainsi $$\\mathbb{P}_{\\theta}(Z = k \\mid X = x) = \\frac{ \\pi_k f_{\\mu_k,\\Sigma_k}(x) }{ \\sum_{l=1}^K \\pi_l f_{\\mu_l,\\Sigma_l}(x) }$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7028ced2",
   "metadata": {},
   "source": [
    "### T4\n",
    "\n",
    "On cherche $\\hat{\\theta} = (\\hat{\\boldsymbol{\\pi}},\\hat{\\boldsymbol{\\mu}},\\hat{\\boldsymbol{\\Sigma}})$ qui maximise la fonction $F$ suivante :\n",
    "\n",
    "$$ F(\\theta) = \\sum_{i=1}^{n}{\\log p_{\\theta}(z_i,x_i)} $$\n",
    "$$ F(\\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) = \\sum_{i=1}^{n}{\\log(\\pi_{k_i})} + \\sum_{i=1}^{n}{\\log(f_{\\mu_i,\\Sigma_i}(x_i))}$$\n",
    "\n",
    "L'influence de $\\boldsymbol{\\pi}$ n'apparaît que dans le premier terme. On peut donc trouver $\\hat{\\boldsymbol{\\pi}}$ en ne s'intéressant pour l'instant qu'à la fonction suivante :\n",
    "\n",
    "$$ G(\\boldsymbol{\\pi}) = \\sum_{i=1}^{n}{\\log(\\pi_{z_i})} $$\n",
    "\n",
    "On simplifie cette expression :\n",
    "\n",
    "$$ \\begin{align*} G(\\boldsymbol{\\pi}) &= \\sum_{k=1}^{K}{ \\sum_{\\substack{ i = 1 \\\\ z_i = k }}^{n}{\\log(\\pi_{z_i})} } \\\\ &= \\sum_{k=1}^{K}{n_k\\log(\\pi_k)} \\end{align*} $$\n",
    "\n",
    "On cherche donc le point de   $ \\{ \\boldsymbol{\\pi} / \\sum_{k=1}^{K} \\pi_k  = 1 \\} $   qui maximise G.\n",
    "\n",
    "En posant C la fonction définie par   $ C(\\boldsymbol{\\pi}) = \\sum_{k=1}^{K} \\pi_k  - 1 $ , on sait que $\\hat{\\boldsymbol{\\pi}}$ vérifie les deux conditions suivantes :\n",
    "\n",
    "$$ \\left\\{ \\begin{align*} \\exists \\lambda, \\nabla G(\\hat{\\boldsymbol{\\pi}}) &= \\lambda \\nabla C(\\hat{\\boldsymbol{\\pi}}) \\\\  C(\\hat{\\boldsymbol{\\pi}}) &= 0 \\end{align*} \\right. $$\n",
    "\n",
    "On pose alors   $ H(\\boldsymbol{\\pi}, \\lambda) = G(\\boldsymbol{\\pi}) - \\lambda C(\\boldsymbol{\\pi}) $  ainsi : \n",
    "\n",
    "$$ \\nabla H(\\boldsymbol{\\pi}, \\lambda) = \\left( \\begin{array}{c} \\nabla G(\\boldsymbol{\\pi}) - \\lambda \\nabla C(\\boldsymbol{\\pi}) \\\\ -C(\\boldsymbol{\\pi})  \\end{array} \\right) $$\n",
    "\n",
    "On cherche donc les solutions de $ \\nabla H(\\boldsymbol{\\pi}, \\lambda) = 0 $\n",
    "\n",
    "\n",
    "Soit $ k \\in \\{1, K\\} $\n",
    "\n",
    "$$\\frac{\\partial H}{\\partial \\pi_k}(\\boldsymbol{\\pi}, \\lambda) = \\frac{n_k}{\\pi_k} - \\lambda $$\n",
    "\n",
    "$$ Donc \\quad \\hat{\\pi_k} = \\frac{n_k}{\\lambda} $$\n",
    "\n",
    "$$Or \\quad \\frac{\\partial H}{\\partial \\lambda}(\\hat{\\boldsymbol{\\pi}}, \\lambda) = \\sum_{k=1}^{K} \\hat{\\pi_k}  - 1 = 0 $$\n",
    "\n",
    "$$Ainsi \\quad 1 = \\sum_{k=1}^{K} \\frac{n_k}{\\lambda} $$\n",
    "\n",
    "$$ \\lambda = \\sum_{k=1}^{K} n_k $$\n",
    "\n",
    "$$ \\lambda = n $$\n",
    "\n",
    "On obtient donc  $ \\hat{\\pi_k} = \\frac{n_k}{n} $  avec  $ n_k = \\sum_{i=1}^{K} \\mathbb{1}_{z_i=k} $\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "On s'intéresse maintenant à trouver $\\hat{\\boldsymbol{\\mu}}$. On sait notamment que :\n",
    "\n",
    "$$ \\begin{align*} 0 = \\frac{\\partial F}{\\partial \\mu_k}(\\hat{\\boldsymbol{\\theta}}) &= \\sum_{\\substack{i=1 \\\\ z_i = k}}^{n}{\\frac{\\partial}{\\partial \\mu_k}(\\log(f_{\\hat{\\mu_k},\\hat{\\Sigma_k}}(x_i)))} \\\\\n",
    "&= \\sum_{\\substack{i=1 \\\\ z_i = k}}^{n}{\\frac{-(x_i-\\hat{\\mu_k}) f_{\\hat{\\mu_k},\\hat{\\Sigma_k}}(x_i)}{f_{\\hat{\\mu_k},\\hat{\\Sigma_k}}(x_i)}} \\\\\n",
    "&= \\sum_{\\substack{i=1 \\\\ z_i = k}}^{n}{\\hat{\\mu_k}-x_i} \\\\\n",
    "&= n_k \\hat{\\mu_k} - \\sum_{\\substack{i=1 \\\\ z_i = k}}^{n}{x_i} \\\\\n",
    "Donc \\quad \\hat{\\mu_k} = \\frac{1}{n_k} \\sum_{i=1}^{n}{\\mathbb{1}_{z_i=k}x_i}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "On procède de même pour déterminer $\\hat{\\boldsymbol{\\Sigma}}$. Pour $\\mu$, $\\Sigma$ et $x$ réels quelconques on a :\n",
    "\n",
    "$$ f_{\\mu, \\Sigma}(x) = \\frac{1}{\\sqrt{2\\pi \\Sigma}}\\exp{-\\frac{(x_\\mu)^2}{2\\Sigma}} $$\n",
    "\n",
    "Ainsi :\n",
    "\n",
    "$$ \\begin{align*} 0 = \\frac{\\partial F}{\\partial \\Sigma_k}(\\hat{\\boldsymbol{\\theta}}) &= \\sum_{\\substack{i=1 \\\\ z_i = k}}^{n}{\\frac{\\partial}{\\partial \\Sigma_k}(\\log(f_{\\hat{\\mu_k},\\hat{\\Sigma_k}}(x_i)))} \\\\\n",
    "&= \\sum_{\\substack{i=1 \\\\ z_i = k}}^{n}{\\frac{1}{f_{\\hat{\\mu_k},\\hat{\\Sigma_k}}(x_i)}}( - \\frac{1}{2\\hat{\\Sigma_k}} + \\frac{(x_i-\\hat{\\mu})^2}{2\\hat{\\Sigma_k}^2}) f_{\\hat{\\mu_k},\\hat{\\Sigma_k}}(x_i) \\\\\n",
    "&= \\frac{1}{2\\hat{\\Sigma_k}}\\sum_{\\substack{i=1 \\\\ z_i = k}}^{n}{\\frac{(x_i-\\mu)^2}{\\hat{\\Sigma_k}} -1} \\\\\n",
    "Donc \\quad \\hat{\\Sigma_k} = \\frac{1}{n_k} \\sum_{i=1}^{n}{\\mathbb{1}_{z_i=k}(x_i-\\mu_k)^2}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9afce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thetaEstimation(data, clusters, k):\n",
    "    if len(clusters) != len(data):\n",
    "        raise ValueError(\"The number of clusters must match the number of data points.\")\n",
    "\n",
    "    weights = np.zeros(k)\n",
    "    means = np.zeros((k, data.shape[1]))\n",
    "    covariances = np.zeros((k, data.shape[1], data.shape[1]))\n",
    "\n",
    "    for cluster in range(k):\n",
    "        idx = np.where(clusters == cluster)[0]\n",
    "        if len(idx) == 0:\n",
    "            means[cluster] = np.zeros(data.shape[1])\n",
    "            covariances[cluster] = np.eye(data.shape[1])\n",
    "            continue\n",
    "        weights[cluster] = len(idx) / len(clusters)\n",
    "        means[cluster] = np.mean(data[idx], axis=0)\n",
    "        covariances[cluster] = np.cov(data[idx].T, bias=True)\n",
    "        \n",
    "\n",
    "    gmm = GaussianMixtureModel(data.shape[1])\n",
    "    gmm.set_params(weights, means, covariances)\n",
    "    return gmm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4573012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustersEstimation(data, theta, k):\n",
    "    clusters = np.zeros(len(data))\n",
    "    vraisemblances = np.zeros(k)\n",
    "    for i in range(len(data)):\n",
    "        point = data[i]\n",
    "        for j in range(k):\n",
    "            vraisemblances[j] = multivariate_normal.pdf(point, mean=theta.means[j], cov=theta.covariances[j]) * theta.weights[j]\n",
    "        clusters[i] = np.argmax(vraisemblances)\n",
    "    return clusters\n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be16d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialEstimation(data, k, loops = 5):\n",
    "    centres = data[:k]\n",
    "    zInitial = np.zeros(len(data))\n",
    "    for i in range(loops):\n",
    "        for i in range(len(data)):\n",
    "            distances = np.zeros(len(centres))\n",
    "            for j in range(len(centres)):\n",
    "                distances[j] = np.linalg.norm(data[i] - centres[j])\n",
    "            if distances[0] <= distances[1] and distances[0] <= distances[2]:\n",
    "                zInitial[i] = 0\n",
    "            elif distances[1] <= distances[2]:\n",
    "                zInitial[i] = 1\n",
    "            else:\n",
    "                zInitial[i] = 2\n",
    "        centres = np.mean(data[zInitial == 0], axis=0), np.mean(data[zInitial == 1], axis=0), np.mean(data[zInitial == 2], axis=0)\n",
    "    return zInitial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab940857",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_skLearn = False\n",
    "PlotClusters = True\n",
    "\n",
    "if Test_skLearn:\n",
    "    data = sklearn.datasets.make_blobs(centers=3, n_samples=1000)[0]\n",
    "else:\n",
    "    gmm = GaussianMixtureModel(d=2)\n",
    "    gmm.set_params(\n",
    "        weights=[0.3, 0.3, 0.4],\n",
    "        means=[[0, 0], [4, 4], [-6.0, 3.0]], \n",
    "        covariances=np.array([[[0.6, 0], [0, 0.6]], [[3.0, 0], [0, 3.0]], [[0.5, 0], [0, 0.5]]])\n",
    "    )\n",
    "    data, Z = plot_multivariate_normal(3, gmm, 1000)\n",
    "    \n",
    "z = initialEstimation(data, 3, 1)\n",
    "theta = thetaEstimation(data, z, 3)\n",
    "\n",
    "meansPrev = data[:3]\n",
    "plt.scatter(meansPrev[:,0], meansPrev[:, 1], color='black', marker='o', label='Means', zorder = 1)\n",
    "\n",
    "for _ in range(30):\n",
    "    plt.scatter(theta.means[:, 0], theta.means[:, 1], color='black', marker='o', label='Means', zorder = 1)\n",
    "    for i in range(3):\n",
    "        plt.arrow(meansPrev[i,0], meansPrev[i, 1], (theta.means[i, 0] - meansPrev[i,0]), (theta.means[i, 1] - meansPrev[i,1]), zorder = 1)\n",
    "    \n",
    "    if _ % 5 == 0 and PlotClusters:\n",
    "        color_map = {0: 'red', 1: 'green', 2: 'blue'}\n",
    "        for i in range(3):\n",
    "            for j in range(len(data)):\n",
    "                if z[j] == i:\n",
    "                    plt.scatter(data[j,0], data[j,1], color=color_map[i], edgecolors = 'none', label=f'Cluster {i}', alpha=0.3, zorder = -1)\n",
    "        plt.show()\n",
    "\n",
    "    meansPrev = theta.means.copy()\n",
    "    theta = thetaEstimation(data, z, 3)\n",
    "    z = clustersEstimation(data, theta, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b585d",
   "metadata": {},
   "source": [
    "# 3 Application à des données réelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dbad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.mixture import GaussianMixture as GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f97df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e14287",
   "metadata": {},
   "source": [
    "### S4.\n",
    "Le nombre d'images de la base de donnée nous est donnée par la première composante de `digits.data.shape` (la seconde indiquant le nombre de pixel dans chaque image)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398aeb9",
   "metadata": {},
   "source": [
    "Ainsi la base de donnée contient 1797 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45948c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(data):\n",
    "    fig, ax = plt.subplots(10, 10, figsize=(8, 8),\n",
    "                           subplot_kw=dict(xticks=[], yticks=[]))\n",
    "    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "    for i, axi in enumerate(ax.flat):\n",
    "        im = axi.imshow(data[i].reshape(8, 8), cmap='binary')\n",
    "        im.set_clim(0, 16)\n",
    "plot_digits(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5114620d",
   "metadata": {},
   "source": [
    "### S5.\n",
    "En toute généralité, la fonction `plot_digits` permet d'afficher selon une grille de 10 par 10 les 100 premières images d'un tableau `data` contenant des images en niveau de gris de 8x8 pixels. En l'occurrence elle permet d'afficher les 100 premières images de chiffre de la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e47cca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = digits.data\n",
    "\n",
    "K_components = np.arange(50, 210, 10)\n",
    "models = [GMM(K, covariance_type='full', random_state=0)\n",
    "          for K in K_components]\n",
    "aics = [model.fit(data).aic(data) for model in models]\n",
    "plt.plot(K_components, aics);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e1efc",
   "metadata": {},
   "source": [
    "### S6.\n",
    "Le bloc ci-dessus permet de calculer le critère d'information d'Aike (AIC) pour différentes valeurs du paramètre $K$. Il permet donc d'évaluer la pertinence d'un modèle de mélange de $K$ gaussienne pour décrire la répartition de la couleur dans cette base d'images de chiffre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
