# Gaussian Mixture Models for Data Generation  
**Applied Mathematics Project – École Polytechnique**

## Overview

This project is part of an **Applied Mathematics course** at **École Polytechnique**. It explores the practical use of **Gaussian Mixture Models (GMMs)** to model and generate complex data distributions. The focus is on a real-world application: **generating synthetic handwritten digits** that statistically resemble real data.

---

## 1. Theoretical Foundation: Gaussian Mixture Models

GMMs are a type of **latent variable model** where each data point is assumed to be generated by one of several Gaussian distributions. The key components are:

- A hidden (latent) variable `Z` indicating the component (cluster) of each data point `X`.
- A finite number of components `K`, each with:
  - Mixing coefficient `π_k`
  - Mean vector `μ_k`
  - Covariance matrix `Σ_k`

The tasks in this part include:

- Deriving the probability density function of the model.
- Implementing a simulation function to generate synthetic data from a GMM.
- Visualizing the generated data, starting with 1D and 2D cases.

---

## 2. Parameter Estimation: EM Algorithm

The next step is to estimate the model parameters from **unlabeled observations** using the **Expectation-Maximization (EM)** algorithm.

- **E-step**: Compute the posterior probabilities of the latent variables.
- **M-step**: Update the parameters (`π`, `μ`, `Σ`) based on these probabilities.

Tasks include:

- Implementing the log-likelihood function.
- Coding the EM steps and running them iteratively.
- Visualizing how cluster centers evolve through iterations.
- Observing convergence behavior and stability of the algorithm.

---

## 3. Application to Real Data

The model is then applied to a real-world dataset:

- Handwritten digits, represented as **8×8 pixel grayscale images** (dimension = 64).
- The `GaussianMixture` class from `sklearn.mixture` is used for fitting.
- The model is trained with various values of `K` and evaluated using log-likelihood.

### Final Task:

- Using the trained model with `K = 110`, generate **100 new synthetic digit images**.
- Visually inspect and analyze the quality and diversity of these generated images.

---

## Learning Objectives

- Understand and implement Gaussian Mixture Models (GMMs).
- Gain experience with latent variable models and statistical inference.
- Apply the **EM algorithm** to real and simulated data.
- Work with **high-dimensional datasets** (image data).
- Bridge theoretical concepts with **practical machine learning techniques**.

---

## Tools & Libraries

- Python
- `NumPy`, `Matplotlib`
- `scipy.stats` (`scipy.stats.multivariate_normal`)
- Jupyter Notebook

---

## Credits

Project based on coursework at **École Polytechnique**, with datasets provided by the instructor.
